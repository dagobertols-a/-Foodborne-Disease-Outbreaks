{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math \n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "%matplotlib inline\n",
    "import seaborn as sns\n",
    "from scipy import stats\n",
    "sns.set()\n",
    "plt.style.use(\"ggplot\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Loading dataframe\n",
    "outbreaks = pd.read_excel(open('FoodData.xlsx','rb'), sheetname=0)\n",
    "#General information about the dataframe\n",
    "outbreaks.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "outbreaks.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "outbreaks.Illnesses= outbreaks.Illnesses.astype('float64')\n",
    "outbreaks.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Renaming columns\n",
    "outbreaks.columns = ['Year', 'Month', 'State', 'Species', 'Serotype/Genotype',\n",
    "       'Status', 'Location', 'Illnesses',\n",
    "       'Hospitalizations', 'Fatalities', 'Food',\n",
    "       'Ingredient']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "outbreaks.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the column Month, each entry  has  a number between  1 and 12. Let us map these numbers  to the name of the months.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Renaming column Month to strings \n",
    "outbreaks[\"Month\"] =outbreaks.Month.map({1:'January', 2:'February', 3:'March', 4:'April', 5:'May', 6:'June', \n",
    "                          7:'July', 8:'August', 9:'September', \\\n",
    " 10:'October', 11:'November', 12:'December'})\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "outbreaks.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's substitute the names of the states to their abbreviations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# States to abbreviations\n",
    "s_state_abbrev = {\n",
    "    'Alabama': 'AL',\n",
    "    'Alaska': 'AK',\n",
    "    'Arizona': 'AZ',\n",
    "    'Arkansas': 'AR',\n",
    "    'California': 'CA',\n",
    "    'Colorado': 'CO',\n",
    "    'Connecticut': 'CT',\n",
    "    'Delaware': 'DE',\n",
    "    'Florida': 'FL',\n",
    "    'Georgia': 'GA',\n",
    "    'Hawaii': 'HI',\n",
    "    'Idaho': 'ID',\n",
    "    'Illinois': 'IL',\n",
    "    'Indiana': 'IN',\n",
    "    'Iowa': 'IA',\n",
    "    'Kansas': 'KS',\n",
    "    'Kentucky': 'KY',\n",
    "    'Louisiana': 'LA',\n",
    "    'Maine': 'ME',\n",
    "    'Maryland': 'MD',\n",
    "    'Massachusetts': 'MA',\n",
    "    'Michigan': 'MI',\n",
    "    'Minnesota': 'MN',\n",
    "    'Mississippi': 'MS',\n",
    "    'Missouri': 'MO',\n",
    "    'Montana': 'MT',\n",
    "    'Nebraska': 'NE',\n",
    "    'Nevada': 'NV',\n",
    "    'New Hampshire': 'NH',\n",
    "    'New Jersey': 'NJ',\n",
    "    'New Mexico': 'NM',\n",
    "    'New York': 'NY',\n",
    "    'North Carolina': 'NC',\n",
    "    'North Dakota': 'ND',\n",
    "    'Ohio': 'OH',\n",
    "    'Oklahoma': 'OK',\n",
    "    'Oregon': 'OR',\n",
    "    'Pennsylvania': 'PA',\n",
    "    'Rhode Island': 'RI',\n",
    "    'South Carolina': 'SC',\n",
    "    'South Dakota': 'SD',\n",
    "    'Tennessee': 'TN',\n",
    "    'Texas': 'TX',\n",
    "    'Utah': 'UT',\n",
    "    'Vermont': 'VT',\n",
    "    'Virginia': 'VA',\n",
    "    'Washington': 'WA',\n",
    "    'West Virginia': 'WV',\n",
    "    'Wisconsin': 'WI',\n",
    "    'Wyoming': 'WY',\n",
    "    'Washington DC' : \"DC\",\n",
    "    'Guam':'GU',\n",
    "    'Puerto Rico' : \"PR\",\n",
    "    'Republic of Palau' : \"PW\",\n",
    "    'Multistate': \"MUL_STATE\"\n",
    "};\n",
    "# Map State_name to its abbreviation\n",
    "State_to_Abbrev = lambda state: s_state_abbrev[state] \n",
    "\n",
    "state_abrev= outbreaks[\"State\"].apply(State_to_Abbrev)\n",
    "outbreaks[\"State\"]  = state_abrev"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "outbreaks.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let us explore the null values for  each column."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sorted list of null values\n",
    "null_values= outbreaks.isnull().sum().sort_values(axis=0, ascending=True)\n",
    "# Percent of not null values by column\n",
    "percent_null_values= 100.0*null_values/outbreaks.shape[0]\n",
    "percent_not_null_values = 100.0- percent_null_values\n",
    "print(\"Not null values by column\")\n",
    "print(\"\")\n",
    "percent_not_null_values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Plot of not null values by column\n",
    "percent_not_null_values.plot(kind=\"bar\",rot=70)\n",
    "plt.title(\"Percent of not null values by column\")\n",
    "plt.ylabel(\"Percent\")\n",
    "#plt.xlabel(\"Column\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Removing columns with many null values($\\approx 50 \\%$ or more )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Removing columns: Serotype/Genotype and Ingredient\n",
    "# Also, let us remove column Status \n",
    "\n",
    "outbreaks= outbreaks[['Year', 'Month', 'State', 'Species',\n",
    "       'Location', \"Food\",'Illnesses', 'Hospitalizations', 'Fatalities']]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's search for hidden white spaces in the categorical columns. When we do a search for a substring in an entry if this  entry is NaN Python give us  an error. To avoid that let us impute a value of type string to the NaN entries. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "str_columns = ['Species', 'Location', 'Food']\n",
    "for column in str_columns:\n",
    "        outbreaks_column= outbreaks[column].fillna(\"null_value\",inplace= False)\n",
    "        del outbreaks[column]\n",
    "        outbreaks[column] = outbreaks_column\n",
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Hidden white spaces."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "       \n",
    "import re\n",
    "\n",
    "def leading_spaces(column):# Looking for leading spaces\n",
    "    begin_spaces=lambda str: True  if re.search(r'^\\s', str) else False\n",
    "    return (outbreaks[column].apply(begin_spaces)).sum()\n",
    "\n",
    "\n",
    "\n",
    "for column in str_columns:\n",
    "    #Trailing\n",
    "    print(\"Number of observations with Leading spaces in column {}: {}\".format(column,leading_spaces(column)) )\n",
    "\n",
    "print(\"\") \n",
    "    \n",
    "def trailing_spaces(column):# Looking for trailing spaces\n",
    "    trailing_spaces=lambda str: True  if re.search(r'\\s$', str) else False\n",
    "    return (outbreaks[column].apply(trailing_spaces)).sum()\n",
    "\n",
    "\n",
    "for column in str_columns:\n",
    "    #Trailing\n",
    "    print(\"Number of observations with trailing spaces in column {}: {}\".format(column,trailing_spaces(column)) )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have an extra trailing space in Food."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "remove_trailing = lambda word: word.strip()\n",
    "outbreaks[\"Food\"] = outbreaks[\"Food\"].apply(remove_trailing)\n",
    "print(\"Number of observations with trailing spaces in Food: {}\"\\\n",
    "      .format(trailing_spaces(\"Food\") )) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Looking for spaces around the semicolons."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Any space before of after the ;\n",
    "for column in str_columns:\n",
    "    print(\"For {}, do all semicolons contain an space after? {}\".format(column,(outbreaks[column].str.contains(\";\")).sum() \\\n",
    "    ==(outbreaks[column].str.contains(\"; \")).sum()))\n",
    "print(\"\")\n",
    "for column in str_columns:\n",
    "    print(\"For {}, number of observations that  contain an space before ; {}\"\\\n",
    "          .format(column,(outbreaks[column].str.contains(\" ;\")).sum()))\n",
    "    print(\"For {}, number of observations that  contain an space before ; {}\"\\\n",
    "          .format(column,(outbreaks[column].str.contains(\" ;\")).sum()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In columns such as Location, Species and Food, there are records with multiple entries.  It can happen that two records contain the same elements but arranged in a different way. To avoid counting these records as different let us sort each record."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataframe = None\n",
    "dataframe= outbreaks.copy()\n",
    "def homogeneous(data,column): \n",
    "    df = data.copy()\n",
    "    str_column=df[column].str.lower().str.strip().str.replace(\"; \",\";\")\n",
    "    sort_list = lambda element: sorted(element) # sort\n",
    "    lst_column= str_column.str.split(\";\").apply(sort_list)# each entry is a list\n",
    "    return str_column, lst_column\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "How many different labels do we have in each categorical column?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def unique_labels(data,column):\n",
    "    df = data.copy()\n",
    "    df[column + \"_list\"]= homogeneous(df,column)[1]\n",
    "    #df[\"Is_There_Hosp\"]=(df.Hospitalizations > 0.0).astype(\"float\")\n",
    "    temp_df = df[[column + \"_list\"]]\n",
    "    \n",
    "    set_values = []\n",
    "    for entry in temp_df[column + \"_list\"]:\n",
    "        for element in entry:\n",
    "            set_values.append(element)\n",
    "    unique= list(set(set_values))\n",
    "    return np.array(unique,dtype= \"O\")\n",
    "\n",
    "\n",
    "\n",
    "unique_labels(dataframe,\"Location\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we have: two locations named: long-term care/nursing home/assisted living facility' and long-term care/nursing home/as...', but they are the same place. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.set_option(\"display.max_colwidth\", 10000)\n",
    "loc_formated = None\n",
    "loc= homogeneous(dataframe,\"Location\")[0]\n",
    "loc_formated=loc.replace('long-term care/nursing home/as...','flag_mark',regex=True,inplace= False)\n",
    "\n",
    "loc_formated_flag=loc_formated.replace('ted living facility','',regex=True,inplace= False)\n",
    "loc_back=loc_formated_flag.replace('flag_mark','long-term care',regex=True,inplace= False)\n",
    "\n",
    "\n",
    "dataframe.loc[:,\"Location\"]= loc_back;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Final result \n",
    "unique_labels(dataframe,\"Location\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Putting column Location in a more readable format."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dict_contain = {\"caterer\":\"caterer\",\"fast-food\": \"rest-fast-food\",\n",
    "               \"banquet\":\"banquet\",\"sit-down\":\"rest-sit-down\",\n",
    "                \"other or unknown\":\"rest-other or unknown\", \"fair\": \"fair or mobile\" }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# From the original location to a more readable format.\n",
    "def replacement(element):\n",
    "    for index,entry in enumerate(element):\n",
    "        for key in dict_contain:\n",
    "            if key in entry:\n",
    "                element[index]=dict_contain[key] \n",
    "       \n",
    "    return element"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.set_option(\"display.max_colwidth\", 100)\n",
    "outbreaks.loc[:,\"Location\"]=homogeneous(dataframe,\"Location\")[1].apply(replacement).apply(lambda x: \"; \".join(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#homogeneous(dataframe,\"Species\")[0]\n",
    "unique_labels(dataframe,\"Species\")    \n",
    "                       "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "outbreaks.loc[:,\"Species\"] = homogeneous(dataframe,'Species')[1].apply(lambda x: \"; \".join(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "outbreaks.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for column in str_columns:\n",
    "    outbreaks.loc[:,column] = outbreaks[column].apply(lambda entry: np.nan if entry ==\"null_value\" else entry)\n",
    "    \n",
    "    \n",
    "outbreaks.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Removing Palau\n",
    "outbreaks= outbreaks.loc[outbreaks.State!=\"PW\",:]\n",
    "outbreaks= outbreaks.reset_index()\n",
    "del outbreaks[\"index\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_outbreaks = outbreaks.copy() # backup\n",
    "raw_outbreaks.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To treat the null values we work with each feature separately. To each null entry we  assign a random value from the set of all possible not null values of the column."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fill_nulls(data,rand_values,column):\n",
    "        dataframe = data.copy()\n",
    "        new_column= dataframe[column].fillna(value=rand_values,inplace= False)\n",
    "        return new_column\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filling null values for each column \n",
    "for column in outbreaks.columns:\n",
    "    if outbreaks[column].isnull().sum()> 0.0:\n",
    "        not_null= outbreaks[~outbreaks[column].isnull()][column]\n",
    "        rand_values_array= np.random.choice(not_null,size=outbreaks[column].shape[0])\n",
    "        rand_values_column=pd.Series(rand_values_array)\n",
    "        outbreaks.loc[:,column]= fill_nulls(outbreaks,rand_values_column,column)\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "outbreaks.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# One hot encoding for unique labels of the categorical columns\n",
    "def col_to_vector(data,column):\n",
    "    dataframe= data.copy()\n",
    "    column_labels= unique_labels(dataframe,column)\n",
    "    column_list_format=dataframe[column].str.split(\"; \")\n",
    "    array_hot= []\n",
    "    for values in column_list_format:\n",
    "        col_vector= np.zeros(column_labels.shape[0])\n",
    "        for index,label in enumerate(column_labels):\n",
    "            for value in values:\n",
    "                if value == label:\n",
    "                    col_vector[index]+=1\n",
    "        array_hot.append(col_vector)\n",
    "    return column_labels, np.array(array_hot)            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vector_species=col_to_vector(outbreaks,\"Species\")[1]\n",
    "vector_location=col_to_vector(outbreaks,\"Location\")[1]\n",
    "df_vector_species = pd.DataFrame(vector_species,columns=col_to_vector(outbreaks,\"Species\")[0])\n",
    "#df_vector_species\n",
    "df_vector_location = pd.DataFrame(vector_species,columns=col_to_vector(outbreaks,\"Species\")[0])\n",
    "df_vector_location= pd.DataFrame(vector_location,columns=col_to_vector(outbreaks,\"Location\")[0])\n",
    "df_vector_location.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#normed_vector_species=1.0*col_to_vector(outbreaks,\"Species\")/col_to_vector(outbreaks,\"Species\").max(axis=0)\n",
    "#normed_vector_location=1.0*col_to_vector(outbreaks,\"Location\")/col_to_vector(outbreaks,\"Location\").max(axis=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Illnesses in log scale."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "outbreaks[\"log_stand_Illnesses\"]=np.log(outbreaks.Illnesses)\n",
    "mean = outbreaks[\"log_stand_Illnesses\"].mean()\n",
    "std = outbreaks[\"log_stand_Illnesses\"].std()\n",
    "median = outbreaks[\"log_stand_Illnesses\"].median()\n",
    "quant_3 = np.percentile(outbreaks[\"log_stand_Illnesses\"],75)\n",
    "quant_1 = np.percentile(outbreaks[\"log_stand_Illnesses\"],25)\n",
    "plt.cla()\n",
    "sns.distplot(np.log(outbreaks.Illnesses), bins=11, color='red',fit=None)\n",
    "plt.title('Distribution of Illnesses in log scale')\n",
    "plt.axvline(x=quant_3,label = \"3-quantile\",c=\"blue\",alpha = 0.6)\n",
    "plt.axvline(x=quant_1, label = \"1-quantile\",c=\"green\",alpha = 0.6)\n",
    "plt.axvline(x=median, label = \"median\",c=\"k\",alpha = 0.6)\n",
    "plt.legend()\n",
    "display()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "By using the median as threshold, let us transform the numerical  column Illnesses to a categorical one."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def A_lot_of_Ill(log_stand_Illnesses,median):\n",
    "    out_A_lot_of_Ill = outbreaks[log_stand_Illnesses] >= median\n",
    "    out_A_lot_of_Ill.astype(\"float\")\n",
    "    return out_A_lot_of_Ill\n",
    "\n",
    "median = outbreaks[\"log_stand_Illnesses\"].median()\n",
    "outbreaks[\"A_lot_of_Ill\"] = A_lot_of_Ill(\"log_stand_Illnesses\",median)\n",
    "outbreaks.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Statistics of hospitalizations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"median for hospitalizations {:.2f}\".format(np.percentile(outbreaks.Hospitalizations,50)))\n",
    "\n",
    "plt.hist(outbreaks.Hospitalizations,bins = 10,range= [0.0,10] )\n",
    "plt.xlabel(\"Hospitalizations\")\n",
    "plt.ylabel(\"Outbreaks\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Percent of outbreaks that not produce hospitalizations {:.1f} %\".format(100.0*(outbreaks.Hospitalizations == 0.0).sum()/outbreaks.Hospitalizations.shape[0]))\n",
    "print(\"Percent of outbreaks that produce hospitalizations {:.1f} %\".format(100.0*(outbreaks.Hospitalizations > 0.0).sum()/outbreaks.Hospitalizations.shape[0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we have a lot of cases with zero hospitalizations not many outbreaks with one or more hospitalizations(in fact they are at the rate of $75/25$)  that mimics a Pareto distribution. As most outbreaks do not generate hospitalizations, let us  divide the dataset in two categories: outbreaks with hospitalizations and outbreaks without them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"median for fatalities {:.2f}\".format(np.percentile(outbreaks.Fatalities,50)))\n",
    "\n",
    "plt.hist(outbreaks.Fatalities,bins = 10,range= [0.0,6] )\n",
    "plt.xlabel(\"Fatalities\")\n",
    "plt.ylabel(\"Outbreaks\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Percent of outbreaks that not produce deaths {:.1f} %\".format(100.0*(outbreaks.Fatalities == 0.0).sum()/outbreaks.Fatalities.shape[0]))\n",
    "print(\"Percent of outbreaks that produce deaths {:.1f} %\".format(100.0*(outbreaks.Fatalities > 0.0).sum()/outbreaks.Fatalities.shape[0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we can see that the outbreaks with fatalities are very rare. The rate of deaths vs  not deaths is  $99/1$. So, essentially we have two classes: outbreaks with deaths and outbreaks without them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def two_classes(data, column,thr=0.0):\n",
    "    #not_null= dataframe[~dataframe[column].isnull()]\n",
    "    #not_null_column= not_null[column]\n",
    "    dataframe= data.copy()\n",
    "    binary_classes= (dataframe[column] > thr).astype(\"float\")\n",
    "    return binary_classes\n",
    "\n",
    "outbreaks[\"Are_there_Hospitalizations\"] = two_classes(outbreaks, \"Hospitalizations\",thr=0.0)\n",
    "outbreaks[\"Are_there_Fatalities\"] = two_classes(outbreaks,\"Fatalities\",  0.0)\n",
    "outbreaks.head()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Behaviour of outbreaks with the years. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "outbreaks_years = outbreaks[[\"Year\",\"Illnesses\"]]\n",
    "# Pandas serie with number of outbreaks by year(serie index)\n",
    "byyear= outbreaks_years.groupby(\"Year\").Illnesses.count()\n",
    "byyear_percent = round(100.0*byyear/byyear.sum(),2);\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Graph of number of Illnesses by year\n",
    "byyear_percent.plot(kind =\"bar\",rot = 70,grid = True)\n",
    "plt.title('Percent of Illnesses by year')\n",
    "plt.ylabel('Percent of illnesses')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It seems that the number of outbreaks decreases with the course of the years."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#def not_null_column(dataframe,column):\n",
    "#    return  dataframe[(~dataframe[column].isnull())]\n",
    "\n",
    "def sort_by_frecuency(data,column):\n",
    "    dataframe = data.copy()\n",
    "    outbreaks_column = pd.pivot_table(dataframe, index=column, values='Illnesses', aggfunc='count')\n",
    "    # Sorted number of outbreaks by state\n",
    "    sorted_by_column= outbreaks_column.sort_values(by=\"Illnesses\",axis=0, ascending=False)\n",
    "    percent_outbreaks_by_column= 100.0*sorted_by_column/sorted_by_column['Illnesses'].sum()\n",
    "    percent_outbreaks_by_column.columns = [\"%_of_outbreaks\"]\n",
    "    return percent_outbreaks_by_column\n",
    "\n",
    "    \n",
    "def most_common(data, column,n_classes=10):\n",
    "    dataframe = data.copy()\n",
    "    top_classes=list(dataframe[column].value_counts().index)[:n_classes]\n",
    "    def common_classes(element):\n",
    "        # This fuction return the  n_classes classes that produced most of the outbreaks\n",
    "        if element in top_classes:\n",
    "            return element \n",
    "        else:\n",
    "            return \"bulk_\" + column\n",
    "    most_common_elements =dataframe[column].apply(common_classes)\n",
    "    return most_common_elements\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def classes_percent(dataframe,column,target, month=False):\n",
    "    outbreaks_our_columns = dataframe[[column,target,'Illnesses']]\n",
    "    outbreaks_each_class= pd.DataFrame(outbreaks_our_columns.\n",
    "                                       groupby([column,target]).Illnesses.count())\n",
    "    outbreaks_each_class.columns = [\"Count\"]\n",
    "    if month:\n",
    "        # Sort the results according to calendar \n",
    "        outbreaks_each_class_sorted=outbreaks_each_class.reindex(['January', 'February', 'March', 'April', 'May', 'June', \n",
    "                          'July', 'August', 'September', 'October', 'November', 'December'],level=0)\n",
    "    else:\n",
    "        outbreaks_each_class_sorted =outbreaks_each_class.copy()\n",
    "    \n",
    "    unstack_each_class= outbreaks_each_class_sorted.unstack().fillna(0.0)\n",
    "    \n",
    "    unstack_each_class.columns =[\"Class_zero\",\"Class_non_zero\"]\n",
    "    #sorted_loc.sort_values(by=\"More_than_zero\", ascending=0,inplace=True)\n",
    "    unstack_each_class_percent=pd.DataFrame(round(100.0*unstack_each_class[\"Class_non_zero\"]/unstack_each_class.sum(axis = 1),2),columns=[\"Class_non_zero\"])\n",
    "    unstack_each_class_percent.sort_values(by=\"Class_non_zero\", ascending=0,inplace=True)\n",
    "    return  unstack_each_class_percent \n",
    "    \n",
    "    \n",
    "    \"\"\"outbreaks_each_class_sorted[\"Percent\"] = 0.0\n",
    "    for indx,lev in enumerate(outbreaks_each_class_sorted.index.levels[0]):\n",
    "        outbreaks_each_class_sorted.loc[lev,:]['Percent'] \\\n",
    "        = round(100.0*outbreaks_each_class_sorted.loc[lev,:].Count\n",
    "                                                /outbreaks_each_class_sorted.sum(level = outbreaks_each_class_sorted.index.names[0]).Count[indx],2)\n",
    "    return #outbreaks_each_class #outbreaks_each_class_sorted\"\"\"\n",
    "    \n",
    "    \n",
    "def baseline(target):\n",
    "    return pd.DataFrame(100.0*outbreaks[target].value_counts()\n",
    "                        /outbreaks[target].value_counts().sum()).loc[1.0,target]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "features = [\"Location\", \"Species\",\"Food\", \"State\"]\n",
    "target = \"Are_there_Hospitalizations\"\n",
    "print(\"Percent of outbreaks that cause hospitalization {}%\"\\\n",
    "      .format(round(baseline(target),2)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In other words, If we randomly select an entry in our dataset, the probability that this observation causes hospitalizations is $26$%. This value is going to define our base model. If we know the location or the species would it  change this probability?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "baseline = baseline(target)\n",
    "for feature in features:\n",
    "    outbreaks[\"Most_common_\" +feature] = most_common(outbreaks, feature,n_classes=10)\n",
    "    percent= classes_percent(outbreaks,\"Most_common_\" +feature,target, month=False)\n",
    "    plt.axhline(y=baseline, color = 'k',label =\"Baseline\")\n",
    "    plt.bar(percent.index, percent.Class_non_zero.values)\n",
    "    plt.ylabel(\"Percent\")\n",
    "    plt.title(\"Percent of outbreaks with Hospitalizations\")\n",
    "    plt.legend()\n",
    "    plt.xticks(rotation=70)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we can see, in general,  the features  Locations and Species are not too bad indicators that if a given  outbreak is going to have Hospitalizations or not."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "percent= classes_percent(outbreaks,'Month',target, month=True)\n",
    "plt.axhline(y=baseline, color = 'k',label =\"Baseline\")\n",
    "plt.bar(percent.index, percent.Class_non_zero.values)\n",
    "plt.ylabel(\"Percent\")\n",
    "plt.title(\"Percent of outbreaks with Hospitalizations\")\n",
    "plt.legend()\n",
    "plt.xticks(rotation=70)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It seems that knowing only the month in which the outbreak occurred is not useful in predicting whether there were hospitalizations(at less compared with other features). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "def outliers_by_month(column):\n",
    "    sns.catplot(x=\"Month\", y=column,jitter=0.2,height=5.0, aspect=7/5.0,data=outbreaks)\n",
    "    plt.xticks(rotation=70)\n",
    "    plt.xlabel(\"Months\")\n",
    "    plt.ylabel(column)\n",
    "    plt.axhline(y=np.percentile(outbreaks[column],99.9), linewidth=0.8, color = 'red',label = \"99.9 percentile\")\n",
    "    plt.legend()\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "str_columns= ['Hospitalizations',\"Illnesses\",\"Fatalities\"]\n",
    "for column in str_columns:\n",
    "    outliers_by_month(column)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since our data is like-Pareto distributed the threshold that we would chose can not be too restrictive. Otherwise, we will lose valuable information in the class that describes the outbreaks that generate hospitalizations. Here we will consider a threshold of 99.9 % for 'Hospitalizations' and \"Illnesses\" and  about 99.95 % for \"Fatalities\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def without_outliers(column,threshold=99.9, fata_threshold=99.9):\n",
    "    if column == \"Fatalities\":\n",
    "        threshold = fata_threshold\n",
    "        \n",
    "    no_outliers=outbreaks[outbreaks[column] <np.percentile(outbreaks[column],threshold)]\n",
    "    sns.catplot(x=\"Month\", y=column,jitter=0.2,height=5.0, aspect=7/5.0,data=no_outliers);\n",
    "    plt.xticks(rotation=70);\n",
    "    plt.xlabel(\"Months\");\n",
    "    plt.ylabel(column);\n",
    "    plt.title(column + \" by month without outliers\")\n",
    "# Let us treat 'Hospitalizations' and \"Illnesses\" together\n",
    "for column in str_columns[:2]:\n",
    "    without_outliers(column,threshold=99.9)\n",
    "# The threshold for  \"Fatalities\" is 99.5%\n",
    "without_outliers(\"Fatalities\",fata_threshold=99.95)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def not_outliers_column(data, train_data, column,threshold=99.9):\n",
    "    if column == \"Fatalities\":\n",
    "        thr = 99.95\n",
    "    else:\n",
    "        thr = threshold\n",
    "        \n",
    "    return (data[column] < np.percentile(train_data[column],thr))\n",
    "\n",
    "features = [\"Hospitalizations\",\"Fatalities\",\"Illnesses\"]\n",
    "not_outliers_outbreaks = outbreaks.copy()\n",
    "for feature in features:\n",
    "    without_outliers= not_outliers_column(not_outliers_outbreaks, not_outliers_outbreaks, feature,threshold=99.9)\n",
    "    not_outliers_outbreaks= not_outliers_outbreaks.loc[without_outliers,:]\n",
    "    \n",
    "not_outliers_outbreaks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "outbreaks= raw_outbreaks.copy()\n",
    "outbreaks.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Separation in train and test sets. The train set is going to be used to train and validate de model. The test set is going to be put aside until the end."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# In order tu use train_test_split we create and extra column filled with ones.\n",
    "extra_column= np.ones(outbreaks.values.shape[0]).reshape(-1,1)\n",
    "X = outbreaks.values\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "# X_train_val is our train-validate set\n",
    "X_train_val, X_test, _, _ = train_test_split(X, extra_column,test_size \\\n",
    "                                                    = 0.25, random_state=3)\n",
    "# outbreaks_train associated dataframe of X_train_val\n",
    "outbreaks_train = pd.DataFrame(X_train_val, columns=outbreaks.columns) \n",
    "# outbreaks_test associated dataframe of X_test\n",
    "outbreaks_test = pd.DataFrame(X_test, columns=outbreaks.columns ) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# scikit-learn k-fold cross-validation\n",
    "from numpy import array\n",
    "from random import seed\n",
    "np.random.seed(12)\n",
    "from sklearn.model_selection import KFold\n",
    "# data \n",
    "data = X_train_val\n",
    "# Prepare cross validation and number of splits\n",
    "n_folds = 3\n",
    "kfold = KFold(n_folds, shuffle=False, random_state=132)\n",
    "# pairs of train and validation sets to do cv\n",
    "data_splits=kfold.split(data)\n",
    "train_sets= [] # n_folds train  sets\n",
    "val_sets = [] # n_folds validation  sets\n",
    "for train_index, val_index in data_splits:\n",
    "    train_sets.append(data[train_index])\n",
    "    val_sets.append(data[val_index])\n",
    "    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To avoid data leaking fron validation set to trainig set, we will do all preprocessing steps in each cv fold."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import confusion_matrix, classification_report\n",
    "#from sklearn.preprocessing import StandardScaler\n",
    "#from sklearn.pipeline import Pipeline\n",
    "#from sklearn.preprocessing import Imputer\n",
    "from sklearn.svm import SVC\n",
    "#from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "#from sklearn.neighbors import KNeighborsClassifier\n",
    "#from sklearn.linear_model import Ridge\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "from sklearn.ensemble import AdaBoostClassifier\n",
    "from sklearn.naive_bayes import (BernoulliNB,GaussianNB,MultinomialNB)\n",
    "from xgboost import XGBClassifier\n",
    "\n",
    "#\n",
    "#check_train=[]\n",
    "#check_val=[]\n",
    "X_train_sets= [] # List that will contain n_folds  clean  train sets \n",
    "X_val_sets= [] # List that will contain n_folds  clean  train sets \n",
    "y_train_sets= []\n",
    "y_val_sets= []\n",
    "for index in range(n_folds):\n",
    "    # Steps in each fold\n",
    "    train = None\n",
    "    val = None\n",
    "    y_train = None\n",
    "    X_train = None\n",
    "    y_val = None\n",
    "    X_val = None\n",
    "    # Train and val sets for this fold\n",
    "    train = train_sets[index]\n",
    "    val = val_sets[index]\n",
    "    # Train and validation dataframes for this fold \n",
    "    train = pd.DataFrame(train, columns=outbreaks_train.columns )    \n",
    "    val = pd.DataFrame(val, columns=outbreaks_train.columns )\n",
    "    \n",
    "    # Formating the numerical features\n",
    "    def to_float(dataframe,column):\n",
    "        data= dataframe.copy()\n",
    "        return data[column].astype(\"float\")\n",
    "\n",
    "    float_columns= ['Illnesses', 'Hospitalizations', 'Fatalities']\n",
    "\n",
    "    for column in float_columns: \n",
    "        train.loc[:,column]= to_float(train,column)\n",
    "        val.loc[:,column]= to_float(val,column)\n",
    "\n",
    "  \n",
    "\n",
    "      \n",
    "    # Logarithmic scale for Illenesses\n",
    "    train[\"Log_Illnesses\"]=np.log(train.Illnesses)\n",
    "    val[\"Log_Illnesses\"]=np.log(val.Illnesses)\n",
    "    \n",
    "    # Median for the train set\n",
    "    global median_Log_Ill_train\n",
    "    median_Log_Ill_train=train[\"Log_Illnesses\"].median()\n",
    "\n",
    "    def A_lot_of_Ill(column):\n",
    "        \"\"\"To evoid data leaking the criteria that define \n",
    "        the clases is calculated only in the train set\"\"\"\n",
    "        \n",
    "        out_A_lot_of_Ill = column >= median_Log_Ill_train\n",
    "        out_A_lot_of_Ill = out_A_lot_of_Ill.astype(\"int\")\n",
    "        return out_A_lot_of_Ill\n",
    "    \"\"\"New columns, two classes: one class describes the outbreaks \n",
    "    with less Illnesses than the median  and the another class \n",
    "    outbreaks with more Illnesses than the median\"\"\"\n",
    "    \n",
    "    train[\"A_lot_of_Ill\"] = A_lot_of_Ill( train[\"Log_Illnesses\"] )\n",
    "    val[\"A_lot_of_Ill\"] = A_lot_of_Ill(val[\"Log_Illnesses\"])\n",
    "    \n",
    "    \n",
    "    # Filling null entries with  values generated randomly from the not null entries.  \n",
    "    def fill_nulls(data,rand_values,column):\n",
    "        \n",
    "          \"\"\"\n",
    "        Let us make a copy of our dataframe to be sure not to change it \n",
    "        while we manipulate and do assignations. \n",
    "        \"\"\"\n",
    "            \n",
    "        dataframe = data.copy()\n",
    "        new_column= dataframe[column].fillna(value= rand_values,inplace= False)\n",
    "        return new_column\n",
    "    \n",
    "    # Filling null values for each column using only train set\n",
    "    for column in train.columns:\n",
    "        if train[column].isnull().sum()> 0.0:\n",
    "            rand_values_train=pd.Series(np.random.choice(train[~train[column].isnull()][column]\\\n",
    "                                       ,size=train[column].shape[0]))\n",
    "            train.loc[:,column]= fill_nulls(train,rand_values_train,column)\n",
    "        if val[column].isnull().sum()> 0.0:\n",
    "            # The values used to fill the validation set come from the train set\n",
    "            rand_values_val=pd.Series(np.random.choice(train[~train[column].isnull()][column]\\\n",
    "                                       ,size=train[column].shape[0]))\n",
    "            val.loc[:,column]= fill_nulls(val,rand_values_val,column)\n",
    "       \n",
    "\n",
    "\n",
    "    #Checkpoint\n",
    "\n",
    "    # Function that produces two classes given a threshold\n",
    "    def two_classes(data, column,thr=0.0):\n",
    "        #not_null= dataframe[~dataframe[column].isnull()]\n",
    "        #not_null_column= not_null[column]\n",
    "        dataframe= data.copy()\n",
    "        binary_classes= (dataframe[column] > thr).astype(\"float\")\n",
    "        return binary_classes\n",
    "    \n",
    "    #Passing Hospitalizations and Fatalities to categorical variables.\n",
    "    train[\"Are_there_Hospitalizations\"] = two_classes(train, \"Hospitalizations\",thr=0.0)\n",
    "    val[\"Are_there_Hospitalizations\"] = two_classes(val, \"Hospitalizations\",thr=0.0)\n",
    "    # New columns \n",
    "    train[\"Are_there_Fatalities\"] = two_classes(train,\"Fatalities\",  0.0)\n",
    "    val[\"Are_there_Fatalities\"] = two_classes(val,\"Fatalities\",  0.0)\n",
    "\n",
    "    \n",
    "    \n",
    "    # Most common  elements for  the features 'Species', 'Location' and  'Food'\n",
    "    def most_common(data, column,top_classes):\n",
    "        dataframe = data.copy()\n",
    "        def common_classes(element):\n",
    "            # This fuction return the most common classes \n",
    "            if element in top_classes:\n",
    "                return element \n",
    "            else:\n",
    "                return \"bulk_\" + column\n",
    "        most_common_elements =dataframe[column].apply(common_classes)\n",
    "        return most_common_elements\n",
    "\n",
    "    categorical_cols = ['Species', 'Location']\n",
    "    n_classes = 30\n",
    "    # New columns with the most common entries\n",
    "    for feature in categorical_cols:\n",
    "        # Most common elements are generated in the train set\n",
    "        top=list(train[feature].value_counts().index)[:n_classes]\n",
    "        train[\"Most_common_\" + feature] = most_common(train,feature,top_classes=top)\n",
    "        val[\"Most_common_\" + feature] = most_common(val,feature,top_classes=top)\n",
    "        top = None\n",
    "\n",
    "    # Outliers \n",
    "    \n",
    "    def not_outliers_column(data, train_set, column,threshold=99.9):\n",
    "        if column == \"Fatalities\":\n",
    "            thr = 99.95\n",
    "        else:\n",
    "            thr = threshold\n",
    "        train_data=train_set.copy()\n",
    "        return (data[column] <= np.percentile(train_data[column],thr))\n",
    "    \n",
    "    features = [\"Hospitalizations\",\"Fatalities\",\"Illnesses\"]\n",
    "    not_outliers_train = train.copy()\n",
    "    not_outliers_val = val.copy()\n",
    "    #print(not_outliers_val.info())\n",
    "    for feature in features:\n",
    "        without_outliers_train= not_outliers_column(not_outliers_train, train\n",
    "                                              , feature,threshold=99.9)\n",
    "        without_outliers_val= not_outliers_column(not_outliers_val, train\n",
    "                                              , feature,threshold=99.9)\n",
    "        temp_not_outliers_train= not_outliers_train.loc[without_outliers_train,:]\n",
    "        temp_not_outliers_val= not_outliers_val.loc[without_outliers_val,:]\n",
    "        not_outliers_train = None\n",
    "        not_outliers_val = None\n",
    "        not_outliers_train = temp_not_outliers_train.loc[:,:]\n",
    "        not_outliers_val = temp_not_outliers_val.loc[:,:]\n",
    "        temp_not_outliers_train = None\n",
    "        temp_not_outliers_val = None\n",
    "        \n",
    "\n",
    "    \n",
    "    #Checkpoint    \n",
    "    #  Clean train and validation set\n",
    "    outbreaks_clasif_train=not_outliers_train[['Year', 'Month', 'State','A_lot_of_Ill', 'Are_there_Hospitalizations',\n",
    "                                               'Species', 'Location']]\n",
    "    outbreaks_clasif_val=not_outliers_val[['Year', 'Month', 'State','A_lot_of_Ill', 'Are_there_Hospitalizations',\n",
    "                                               'Species', 'Location']]\n",
    "    \n",
    "    \n",
    "    \"\"\"outbreaks_clasif_train=not_outliers_train[['Year', 'Month', 'State','A_lot_of_Ill', 'Are_there_Hospitalizations', 'Are_there_Fatalities',\n",
    "                                               'Most_common_Species', 'Most_common_Location']]\n",
    "    outbreaks_clasif_val=not_outliers_val[['Year', 'Month', 'State','A_lot_of_Ill', 'Are_there_Hospitalizations', 'Are_there_Fatalities',\n",
    "                                               'Most_common_Species', 'Most_common_Location']]\n",
    "    \n",
    "    \"\"\"\n",
    "    # pd.get_dummies need that all data have to be in the same dataframe\n",
    "    \n",
    "    # Encoding categorical variables using get_dummies\n",
    "    def get_dummies(data1,data2):\n",
    "        dframe1= data1.copy()\n",
    "        dframe2= data2.copy()\n",
    "        join=pd.concat([dframe1,dframe2],axis = 0) # concatenating train and val sets\n",
    "        join_reindex= join.reset_index(drop=True,inplace=False)\n",
    "        dummies= pd.get_dummies(join_reindex,drop_first=True)\n",
    "        df1_dummies= dummies.iloc[:dframe1.shape[0],:]\n",
    "        df2_dummies = dummies.iloc[dframe1.shape[0]:,:].reset_index(drop=True,inplace=False)\n",
    "        return df1_dummies, df2_dummies\n",
    "\n",
    "    \"\"\"outbreaks_clasif_train.reset_index(inplace = True) \n",
    "    outbreaks_clasif_val.reset_index(inplace = True) \n",
    "    \n",
    "\n",
    "    join_data= pd.concat([outbreaks_clasif_train, outbreaks_clasif_val],ignore_index=True)\n",
    "    \n",
    "    outbreaks_dummies =pd.get_dummies(join_data,drop_first=True)\n",
    "    outbreaks_dummies_train= outbreaks_dummies.iloc[:outbreaks_clasif_train.index[-1]+1,:]\n",
    "    outbreaks_dummies_val = outbreaks_dummies.iloc[outbreaks_clasif_train.index[-1]+1:,:]\n",
    "    \"\"\"\n",
    "    \n",
    "    outbreaks_dummies_train= get_dummies(outbreaks_clasif_train,outbreaks_clasif_val)[0]\n",
    "    outbreaks_dummies_val = get_dummies(outbreaks_clasif_train,outbreaks_clasif_val)[1]\n",
    "    \n",
    "    # Separating predictors from the response variable for train and val sets\n",
    "    y_train = outbreaks_dummies_train['Are_there_Hospitalizations'].values.reshape(-1,1).ravel()\n",
    "    X_train = outbreaks_dummies_train.drop('Are_there_Hospitalizations', axis=1).values\n",
    "    y_val = outbreaks_dummies_val['Are_there_Hospitalizations'].values.reshape(-1,1).ravel()\n",
    "    X_val = outbreaks_dummies_val.drop('Are_there_Hospitalizations', axis=1).values\n",
    "    \n",
    "  \n",
    "    #Clean and  set data. n_folds pairs of train and val sets\n",
    "    X_train_sets.append(X_train)\n",
    "    y_train_sets.append(y_train)\n",
    "    X_val_sets.append(X_val)\n",
    "    y_val_sets.append(y_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.svm import SVC\n",
    "from imblearn.combine import SMOTEENN, SMOTETomek\n",
    "from imblearn.under_sampling import RepeatedEditedNearestNeighbours,TomekLinks,AllKNN,NeighbourhoodCleaningRule,OneSidedSelection\n",
    "from sklearn.metrics import confusion_matrix, classification_report\n",
    "from imblearn.metrics import classification_report_imbalanced\n",
    "from itertools import product\n",
    "    \n",
    "def classifier(method,parameters):\n",
    "    return method(**parameters)\n",
    "\n",
    "\n",
    "# This function takes a dictionary of hyperparameters  and \n",
    "#return all posible combination of them \n",
    "def param_grid(dict_parameters):\n",
    "    hyperparameter_combinations=[]\n",
    "    params =[]\n",
    "    param_names =[]\n",
    "    for key,value in dict_parameters.items():\n",
    "        param_names.append(key)\n",
    "        params.append(value)\n",
    "    combination_params = list(product(*params))# All combinations\n",
    "    for combination in combination_params:\n",
    "        hyperparameter= dict(zip(param_names,combination))\n",
    "        hyperparameter_combinations.append(hyperparameter)\n",
    "    return hyperparameter_combinations\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def metrics(y,y_pred):\n",
    "    classif_report= classification_report(y, y_pred,digits=5)\n",
    "    \"\"\"\n",
    "    Extract all float values. The first 4 elements are associated to class 0\n",
    "    and the labels are as follow: class, presition, recall , f1_score\n",
    "    For class1 is the same. \n",
    "    The last 3 element are the average\n",
    "    \n",
    "    \"\"\"\n",
    "    metrics_classes = re.findall(\"\\d+\\.\\d+\",classif_report)\n",
    "    class0_str= metrics_classes[:4] # class0, class, pres, recal, f1_score\n",
    "    class1_str = metrics_classes[4:8] # class 1\n",
    "    avg_str = metrics_classes[8:]# average\n",
    "    class0= list(map(lambda x: float(x),class0_str))# string to float\n",
    "    class1= list(map(lambda x: float(x),class1_str))\n",
    "    avg=list(map(lambda x: float(x),avg_str))\n",
    "    return class0,class1,avg\n",
    "\n",
    "# Grid search\n",
    "\n",
    "def sampling(X_set,y_set, balanced=False):\n",
    "    X_data, y_data = X_set.copy(),y_set.copy()\n",
    "    x_res, y_res = None,None\n",
    "    sm = SMOTEENN(ratio = \"auto\")\n",
    "        \n",
    "    if balanced== True:\n",
    "        #Sampling and balancing the train data\n",
    "        x_res, y_res = sm.fit_sample(X_data, y_data)\n",
    "    else:\n",
    "        x_res, y_res= X_data, y_data\n",
    "    return x_res, y_res, 100.0*y_res.sum()/y_res.shape[0]\n",
    "\n",
    "       \n",
    "def scores_grid(method=BernoulliNB, dict_parameters={'alpha': [1]}):\n",
    "    class0_metrics =[]\n",
    "    class1_metrics=[]\n",
    "    \n",
    "    #parameters = []\n",
    "    for param_set in param_grid(dict_parameters):\n",
    "        #parameters.append(param_set)\n",
    "        clf= classifier(method,param_set)\n",
    "        \n",
    "        sum_scores_recall_train_c1, sum_scores_recall_val_c1= (0,0)\n",
    "        sum_scores_f1_train_c1, sum_scores_f1_val_c1= (0,0)\n",
    "        \n",
    "        sum_scores_recall_train_c0, sum_scores_recall_val_c0= (0,0)\n",
    "        sum_scores_f1_train_c0, sum_scores_f1_val_c0= (0,0)\n",
    "        \n",
    "        for fold in range(n_folds):\n",
    "            \n",
    "            X_train, y_train, X_val, y_val =None,None,None,None\n",
    "            y_pred_train,y_pred_val,val_scores,train_scores = None,None,None,None\n",
    "            \n",
    "            X_train, y_train = X_train_sampled[fold], y_train_sampled[fold]\n",
    "            X_val, y_val= X_val_sets[fold], y_val_sets[fold]\n",
    "                \n",
    "            clf.fit(X_train, y_train)\n",
    "            y_pred_train = clf.predict(X_train_sets[fold])\n",
    "            y_pred_val = clf.predict(X_val)\n",
    "            \n",
    "            val_scores = metrics(y_val,y_pred_val)\n",
    "            train_scores = metrics(y_train_sets[fold],y_pred_train)\n",
    "            ################## class 1\n",
    "            #recall\n",
    "            sum_scores_recall_val_c1 +=  val_scores[1][2] \n",
    "            sum_scores_recall_train_c1 +=  train_scores[1][2]\n",
    "            \n",
    "            #f1\n",
    "            sum_scores_f1_val_c1 +=val_scores[1][3]\n",
    "            sum_scores_f1_train_c1 +=train_scores[1][3]\n",
    "            \n",
    "            ################# class 0\n",
    "            #Recall\n",
    "            sum_scores_recall_val_c0 +=  val_scores[0][2]\n",
    "            sum_scores_recall_train_c0 +=  train_scores[0][2]\n",
    "            #f1\n",
    "            sum_scores_f1_train_c0 +=train_scores[0][3]\n",
    "            sum_scores_f1_val_c0 +=val_scores[0][3]\n",
    "            \n",
    "        \n",
    "        #Class0\n",
    "        \n",
    "        #Recall\n",
    "        recall_val_c0 = 100.0*sum_scores_recall_val_c0/n_folds\n",
    "        recall_train_c0 = 100.0*sum_scores_recall_train_c0/n_folds\n",
    "        #f1\n",
    "        f1_val_c0 = 100.0*sum_scores_f1_val_c0/n_folds\n",
    "        f1_train_c0 = 100.0*sum_scores_f1_train_c0/n_folds\n",
    "        \n",
    "        class0_metrics.append([recall_train_c0,recall_val_c0,\\\n",
    "                               f1_train_c0,f1_val_c0,param_set])\n",
    "        \n",
    "        \n",
    "        \n",
    "        #Class1\n",
    "        #Recall\n",
    "        recall_val_c1 = 100.0*sum_scores_recall_val_c1/n_folds\n",
    "        recall_train_c1 = 100.0*sum_scores_recall_train_c1/n_folds\n",
    "        #f1\n",
    "        f1_val_c1 = 100.0*sum_scores_f1_val_c1/n_folds\n",
    "        f1_train_c1 = 100.0*sum_scores_f1_train_c1/n_folds\n",
    "        \n",
    "        class1_metrics.append([recall_train_c1,recall_val_c1,\\\n",
    "                                f1_train_c1,f1_val_c1,param_set])\n",
    "        \n",
    "        \n",
    "        \n",
    "        \n",
    "    return np.array(class0_metrics), np.array(class1_metrics)\n",
    "\n",
    "\n",
    "\n",
    "def metrics_dataframe(metrics_class):\n",
    "    # Dataframe with the metrics values \n",
    "    metrics_df=pd.DataFrame(metrics_class[:,:-1],\\\n",
    "                            columns=[\"recall_train\",\"recall_val\",\"f1_train\",\"f1_val\"])\n",
    "    # Parameters values\n",
    "    param_dicts=metrics_class[:,-1]\n",
    "    # Dataframe  of parameters\n",
    "    parameters_df= pd.DataFrame.from_dict(list(param_dicts))\n",
    "    return pd.concat([parameters_df,metrics_df],axis=1)\n",
    "\n",
    "\n",
    "\n",
    "def plot_metric_behaviour(dataframe,var_param,metric,fixed_param=None,step=3):\n",
    "    df_metrics = dataframe.copy()\n",
    "    # Plots of var_param vs metric for different values of fixed_param\n",
    "    if fixed_param != None:\n",
    "        for fixed_p in df_metrics[fixed_param].unique()[::step]:\n",
    "            temp_fixed_p = None\n",
    "            temp_fixed_p= df_metrics[df_metrics[fixed_param]==fixed_p]\n",
    "            label=  fixed_param  + \"_ \" + str(fixed_p)\n",
    "            plt.plot(temp_fixed_p[var_param],temp_fixed_p[metric],label=label)\n",
    "        \n",
    "        \n",
    "    else:\n",
    "        \n",
    "        plt.plot(df_metrics[var_param],df_metrics[metric],label=var_param)\n",
    "        \n",
    "    plt.xlabel(var_param)\n",
    "    plt.ylabel(metric + \" (%)\")\n",
    "    plt.title(metric)\n",
    "    plt.legend()\n",
    "    plt.show()\n",
    "    \n",
    "\n",
    "def plot_learning_curve(dataframe,var_param, metric,fixed_param=None,value=None):\n",
    "    df_metrics = dataframe.copy()\n",
    "    #temp_fixed_p = None\n",
    "    if value != None:\n",
    "        temp_fixed_p= df_metrics[df_metrics[fixed_param]==value]\n",
    "        title= metric + \" with \" + fixed_param + \" = \" + str(value)\n",
    "        # Train metric\n",
    "        plt.plot(temp_fixed_p[var_param],temp_fixed_p[metric +\"_train\" ],label=\"train\")\n",
    "        # Validation metric\n",
    "        plt.plot(temp_fixed_p[var_param],temp_fixed_p[metric +\"_val\" ],label=\"val\")\n",
    "\n",
    "    else:\n",
    "        title= metric \n",
    "        # Train metric\n",
    "        plt.plot(df_metrics[var_param],df_metrics[metric +\"_train\" ],label=\"train\")\n",
    "        # Validation metric\n",
    "        plt.plot(df_metrics[var_param],df_metrics[metric +\"_val\" ],label=\"val\")\n",
    "\n",
    "        \n",
    "        \n",
    "    plt.xlabel(var_param)\n",
    "    plt.ylabel(metric + \" (%)\")\n",
    "    plt.title(title)\n",
    "    plt.legend()\n",
    "    plt.show()\n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(123)\n",
    "global X_train_sampled\n",
    "global y_train_sampled\n",
    "# Sampled train sets \n",
    "X_train_sampled, y_train_sampled =[],[]\n",
    "ratio_list = []\n",
    "np.random.seed(123)\n",
    "for fold in range(n_folds):\n",
    "    X_temp,y_temp=None,None\n",
    "    X_temp,y_temp,ratio=sampling(X_train_sets[fold],y_train_sets[fold], balanced=True)\n",
    "    X_train_sampled.append(X_temp)\n",
    "    y_train_sampled.append(y_temp)\n",
    "    ratio_list.append(ratio)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Random forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_estimators =list(map(np.int, np.logspace(1,2.3,15)))\n",
    "max_depth =list(map(np.int, np.linspace(8,40,15)))\n",
    "rf= scores_grid(RandomForestClassifier,{'n_estimators':n_estimators ,'max_depth': max_depth})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rf_metrics= metrics_dataframe(rf[0])\n",
    "rf_metrics.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "How metrics change with the parameters "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_metric_behaviour(rf_metrics,\"max_depth\", \"recall_train\",\"n_estimators\",2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Learning curves"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_learning_curve(rf_metrics,\"n_estimators\", \"recall\",\"max_depth\",26)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Naive Bayes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "params= np.linspace(0.01,1.0,10)\n",
    "ber= scores_grid(BernoulliNB,{\"alpha\":params})\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ber_metrics= metrics_dataframe(ber[0])\n",
    "ber_metrics.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_metric_behaviour(ber_metrics, \"alpha\", \"f1_val\",None,2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_learning_curve(ber_metrics,\"alpha\", \"recall\",None,None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Cs= np.linspace(0.1,10.0,20)\n",
    "\n",
    "plty= [\"l1\",\"l2\"]\n",
    "lreg= scores_grid(LogisticRegression,{\"penalty\":plty,\"C\":Cs})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lreg_metrics= metrics_dataframe(lreg[0])\n",
    "lreg_metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_metric_behaviour(lreg_metrics,\"C\", \"f1_val\",\"penalty\",1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_learning_curve(lreg_metrics,\"C\", \"f1\",\"penalty\",\"l1\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import svm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Cs= np.linspace(0.5,1.0,2)\n",
    "#'linear',\n",
    "krnl=['linear']#\"rbf\"]\n",
    "sv= scores_grid(svm.SVC,{\"kernel\":krnl,\"C\":Cs})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sv_metrics= metrics_dataframe(sv[0])\n",
    "sv_metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_metric_behaviour(sv_metrics,\"C\", \"f1_val\",\"kernel\",1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_learning_curve(sv_metrics,\"C\", \"f1\",\"kernel\",'linear')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_set = outbreaks_test.copy()\n",
    "train_set = outbreaks_train.copy()\n",
    "train_set.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "del test_set[\"Food\"]\n",
    "del train_set[\"Food\"]\n",
    "test_set.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for column in train_set.columns:\n",
    "    if train_set[column].isnull().sum()> 0.0:\n",
    "        rand_values=pd.Series(np.random.choice(train_set[~train_set[column].isnull()][column]\\\n",
    "                                       ,size=train_set[column].shape[0]))\n",
    "        train_set.loc[:,column]= fill_nulls(train_set,rand_values,column)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_set.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_set.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_set.dropna(inplace=True)\n",
    "test_set.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"for column in test_set.columns:\n",
    "    if test_set[column].isnull().sum()> 0.0:\n",
    "        rand_values=pd.Series(np.random.choice(test_set[~test_set[column].isnull()][column]\\\n",
    "                                       ,size=test_set[column].shape[0]))\n",
    "        test_set.loc[:,column]= fill_nulls(test_set,rand_values,column)\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "       \n",
    "numeric_cols= ['Illnesses', 'Hospitalizations', 'Fatalities']\n",
    "\n",
    "for column in numeric_cols:\n",
    "    train_set.loc[:,column]=train_set[column].astype(\"float64\",copy=False)\n",
    "    test_set.loc[:,column]=test_set[column].astype(\"float64\",copy=False)\n",
    "test_set.info()    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_set[\"Log_Illnesses\"]=np.log(train_set.Illnesses)\n",
    "test_set[\"Log_Illnesses\"]=np.log(test_set.Illnesses)\n",
    "median_Log_Ill_train=train_set[\"Log_Illnesses\"].median()\n",
    "median_Log_Ill_test=test_set[\"Log_Illnesses\"].median()\n",
    "def A_lot_of_Ill(column,median_Log):\n",
    "    out_A_lot_of_Ill = column >= median_Log\n",
    "    out_A_lot_of_Ill = out_A_lot_of_Ill.astype(\"int\")\n",
    "    return out_A_lot_of_Ill\n",
    "train_set[\"A_lot_of_Ill\"] = A_lot_of_Ill( train_set[\"Log_Illnesses\"],median_Log_Ill_train)\n",
    "test_set[\"A_lot_of_Ill\"] = A_lot_of_Ill( test_set[\"Log_Illnesses\"],median_Log_Ill_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_set.drop(columns=[\"Log_Illnesses\"],inplace=True)\n",
    "test_set.drop(columns=[\"Log_Illnesses\"],inplace=True)\n",
    "train_set.drop(columns=[\"Illnesses\"],inplace=True)\n",
    "test_set.drop(columns=[\"Illnesses\"],inplace=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "test_set.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Passing Hospitalizations and Fatalities to categoricals variables.\n",
    "train_set[\"Are_there_Hospitalizations\"] = two_classes(train_set,\\\n",
    "                                                      \"Hospitalizations\",thr=0.0).astype(\"int\")\n",
    "test_set[\"Are_there_Hospitalizations\"] = two_classes(test_set, \"Hospitalizations\",thr=0.0).astype(\"int\")\n",
    "# New columns \n",
    "train_set[\"Are_there_Fatalities\"] = two_classes(train_set,\"Fatalities\",  0.0).astype(\"int\")\n",
    "test_set[\"Are_there_Fatalities\"] = two_classes(test_set,\"Fatalities\",  0.0).astype(\"int\")\n",
    "\n",
    "train_set.drop(columns=[\"Hospitalizations\"],inplace=True)\n",
    "test_set.drop(columns=[\"Hospitalizations\"],inplace=True)\n",
    "train_set.drop(columns=[\"Fatalities\"],inplace=True)\n",
    "test_set.drop(columns=[\"Fatalities\"],inplace=True)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_set.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dummies_train_set= get_dummies(train_set,test_set)[0]\n",
    "dummies_test_set= get_dummies(train_set,test_set)[1]\n",
    "    \n",
    "# Separating predictors from the response variables for train and val sets\n",
    "y_tr = dummies_train_set['Are_there_Hospitalizations'].values.reshape(-1,1).ravel()\n",
    "X_tr = dummies_train_set.drop('Are_there_Hospitalizations', axis=1).values\n",
    "y_tst = dummies_test_set['Are_there_Hospitalizations'].values.reshape(-1,1).ravel()\n",
    "X_tst =dummies_test_set.drop('Are_there_Hospitalizations', axis=1).values\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n",
    "sm = SMOTEENN(random_state=123, ratio = \"auto\")\n",
    "x_train_sm, y_train_sm = sm.fit_sample(X_tr, y_tr)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reg= RandomForestClassifier(n_estimators=10 ,max_depth=40) \n",
    "#LogisticRegression(C=0.8,penalty='l2')\n",
    "#reg.fit(x_train_sm, y_train_sm)\n",
    "reg.fit(x_train_sm, y_train_sm)\n",
    "# Predict the labels of the test set: y_pred\n",
    "y_pred = reg.predict(X_tst)\n",
    "\n",
    "    # Compute and print metrics\n",
    "#print(\"Accuracy: {}\".format(reg.score(X_tst, y_tst)))\n",
    "    #print(classification_report(y_test, y_pred))\n",
    "    #print(\"Tuned Model Parameters: {}\".format(s.best_params_))\n",
    "    #print(confusion_matrix(y_tst, y_pred)\n",
    "print(classification_report(y_tst, y_pred,digits=2))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"from imblearn.over_sampling import SMOTE  # or: import RandomOverSampler\n",
    "from imblearn.pipeline import Pipeline as imbPipeline\n",
    "\n",
    "from sklearn.datasets import make_classification\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.model_selection import (GridSearchCV,\n",
    "                                     train_test_split,\n",
    "                                     StratifiedKFold)\n",
    "\n",
    "# Generate some data with an 8-to-2 class imbalance.\n",
    "\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(training_features, training_target.ravel(), test_size=0.33, random_state=444);\n",
    "\n",
    "# This doesn't work with sklearn.pipeline.Pipeline because\n",
    "# RandomOverSampler doesn't have a .tranform() method.\n",
    "# (It has .fit_sample() or .sample().\n",
    "pipe = imbPipeline([\n",
    "    ('oversample', SMOTE(random_state=12, ratio = \"auto\")),\\\n",
    "    ('clf', LogisticRegression())]);\n",
    "\n",
    "pipe = imbPipeline([\n",
    "    ('oversample', SMOTE(random_state=444,ratio = 0.4)),\n",
    "    ('clf', XGBClassifier(objective='binary:logistic',subsample=1,colsample_bytree=0.8,seed=123))\n",
    "    ]) \n",
    "\n",
    "max_depth=list(range(2,20,2))\n",
    "learning_rate=[0.01,0.1,0.5,1.0,2.0,3.0,5.0,8.0,10.0]\n",
    "n_estimators=list(range(2,20))\n",
    "#colsample_bytree=[0.01,0.05,0.1,0.3,0.6,0.8,0.9]\n",
    "cs= [0.01,0.05,0.1,0.5,1.0,5.0,10.0,20.0]\n",
    "param_grid = {'clf__C': cs, 'clf__penalty': ['l1', 'l2']}\n",
    "#param_grid = {'clf__alpha':[0.001,0.05,0.1,0.2,0.3,0.5,0.7,0.8,1.0] }\n",
    "#param_grid = {'clf__max_depth' : max_depth,\"clf__learning_rate\":learning_rate,\"clf__n_estimators\": n_estimators}\n",
    "skf = 5 #StratifiedKFold()\n",
    "#param_grid = {'clf__max_depth': [25, 40],'clf__max_features': ['sqrt', 'log2']}\n",
    "\n",
    "grid = GridSearchCV(pipe, param_grid, return_train_score=False,\\\n",
    "                    verbose=False, scoring=\"recall\", cv=5);\n",
    "grid.fit(X_train, y_train);\n",
    "\n",
    "print(grid.best_params_)\n",
    "print(\"\")\n",
    "print(\"Scores for training data\")\n",
    "y_pred_train = grid.predict(training_features_org)\n",
    "print(classification_report(training_target_org, y_pred_train))\n",
    "print(\"Scores for test data\")\n",
    "y_pred = grid.predict(test_features_org)\n",
    "print(classification_report(test_target_org, y_pred))\n",
    "\n",
    "print(grid.score(X_test, y_test))\n",
    "\"\"\";\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
